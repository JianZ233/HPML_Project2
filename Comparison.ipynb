{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7fc6be5-587a-4560-9da0-61f7a66f02c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model_loader import ShardedFP8ModelLoader\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37946fe0-4278-4f82-8198-5f94ea14ce63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to print memory usage\n",
    "\n",
    "def print_memory_usage():\n",
    "    allocated = torch.cuda.memory_allocated()\n",
    "    reserved = torch.cuda.memory_reserved()\n",
    "    print(f\"Memory Allocated: {allocated / 1e6:.2f} MB\")\n",
    "    print(f\"Memory Reserved: {reserved / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5563c44-238e-4918-a2fc-c79f0e46fc4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ByteVerification...\n",
      "Found 0 shards: []\n",
      "Error loading model: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 21.95 GiB of which 40.12 MiB is free. Including non-PyTorch memory, this process has 21.90 GiB memory in use. Of the allocated memory 21.72 GiB is allocated by PyTorch, and 1.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 21.95 GiB of which 40.12 MiB is free. Including non-PyTorch memory, this process has 21.90 GiB memory in use. Of the allocated memory 21.72 GiB is allocated by PyTorch, and 1.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      4\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      5\u001b[0m fp16_loader \u001b[38;5;241m=\u001b[39m ShardedFP8ModelLoader(\n\u001b[1;32m      6\u001b[0m     model_dir\u001b[38;5;241m=\u001b[39mfp16_model_dir,\n\u001b[1;32m      7\u001b[0m     device_ids\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m      8\u001b[0m     memory_efficient\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      9\u001b[0m     fp8_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# None for FP16\u001b[39;00m\n\u001b[1;32m     10\u001b[0m )\n\u001b[0;32m---> 11\u001b[0m \u001b[43mfp16_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Replace with the actual model loading method\u001b[39;00m\n\u001b[1;32m     12\u001b[0m fp16_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFP16 Model Loading Time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfp16_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/project/model_loader.py:88\u001b[0m, in \u001b[0;36mShardedFP8ModelLoader.load_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     85\u001b[0m         torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Ensure no meta parameters remain before distributing layers\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_no_meta_left\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprimary_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Distribute model across GPUs\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribute_layers(model)\n",
      "File \u001b[0;32m~/project/model_loader.py:128\u001b[0m, in \u001b[0;36mShardedFP8ModelLoader._ensure_no_meta_left\u001b[0;34m(self, model, device)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m full_name, param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(model\u001b[38;5;241m.\u001b[39mnamed_parameters()):\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mis_meta:\n\u001b[1;32m    127\u001b[0m         real_param \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(\n\u001b[0;32m--> 128\u001b[0m             \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    129\u001b[0m             requires_grad\u001b[38;5;241m=\u001b[39mparam\u001b[38;5;241m.\u001b[39mrequires_grad\n\u001b[1;32m    130\u001b[0m         )\n\u001b[1;32m    131\u001b[0m         parent \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m    132\u001b[0m         parts \u001b[38;5;241m=\u001b[39m full_name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 21.95 GiB of which 40.12 MiB is free. Including non-PyTorch memory, this process has 21.90 GiB memory in use. Of the allocated memory 21.72 GiB is allocated by PyTorch, and 1.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Initialize FP16 Loader\n",
    "\n",
    "fp16_model_dir = \"/home/jz3607/.llama/checkpoints/Llama3.1-8B-Instruct\"\n",
    "start_time = time.time()\n",
    "fp16_loader = ShardedFP8ModelLoader(\n",
    "    model_dir=fp16_model_dir,\n",
    "    device_ids=[0, 1],\n",
    "    memory_efficient=True,\n",
    "    fp8_format=None  # None for FP16\n",
    ")\n",
    "fp16_loader.load_model()  # Replace with the actual model loading method\n",
    "fp16_time = time.time() - start_time\n",
    "\n",
    "print(f\"FP16 Model Loading Time: {fp16_time:.2f} seconds\")\n",
    "print(\"FP16 Memory Usage:\")\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ede608-a6d5-4da4-bd4c-e6a497e053e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize FP8 Loader\n",
    "fp8_model_dir = \"./Meta-Llama-3.1-8B-Instruct-FP8\"\n",
    "start_time = time.time()\n",
    "fp8_loader = ShardedFP8ModelLoader(\n",
    "    model_dir=fp8_model_dir,\n",
    "    device_ids=[0, 1],\n",
    "    memory_efficient=True,\n",
    "    fp8_format=FP8Format(e4m3=True)\n",
    ")\n",
    "fp8_loader.load_model()  # Replace with the actual model loading method\n",
    "fp8_time = time.time() - start_time\n",
    "\n",
    "print(f\"FP8 Model Loading Time: {fp8_time:.2f} seconds\")\n",
    "print(\"FP8 Memory Usage:\")\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e057bead-0b8f-479d-8875-9c152e83781e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"FP16 Loading Time: {fp16_time:.2f} seconds\")\n",
    "print(f\"FP8 Loading Time: {fp8_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df6e8a3-db8e-4844-9e7c-63c99e10e7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a forward pass for both FP16 and FP8\n",
    "input_data = torch.randn(1, 3, 224, 224).to('cuda')  # Example input tensor\n",
    "\n",
    "# FP16 Forward Pass\n",
    "fp16_output = fp16_loader.model(input_data)\n",
    "\n",
    "# FP8 Forward Pass\n",
    "fp8_output = fp8_loader.model(input_data)\n",
    "\n",
    "# Visualize differences\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(fp16_output.flatten().cpu().detach().numpy(), label=\"FP16 Output\")\n",
    "plt.plot(fp8_output.flatten().cpu().detach().numpy(), label=\"FP8 Output\")\n",
    "plt.legend()\n",
    "plt.title(\"Output Comparison between FP16 and FP8\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adef62f-83e0-4845-a70c-0e7d9ee356ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"Precision\": [\"FP16\", \"FP8\"],\n",
    "    \"Loading Time (s)\": [fp16_time, fp8_time],\n",
    "    \"Memory Allocated (MB)\": [\n",
    "        torch.cuda.memory_allocated(device=0) / 1e6,\n",
    "        torch.cuda.memory_allocated(device=1) / 1e6\n",
    "    ],\n",
    "    \"Memory Reserved (MB)\": [\n",
    "        torch.cuda.memory_reserved(device=0) / 1e6,\n",
    "        torch.cuda.memory_reserved(device=1) / 1e6\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00be48cc-5c72-4130-8ed7-4b47af3610d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison\n",
    "\n",
    "df_results.plot(x=\"Precision\", y=[\"Loading Time (s)\", \"Memory Allocated (MB)\", \"Memory Reserved (MB)\"], kind=\"bar\", figsize=(12, 6))\n",
    "plt.title(\"FP16 vs FP8 Comparison\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
